{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "\n",
    "from lgn.models import LGNCG, LGNTopTag\n",
    "from lgn.models.autotest import lgn_tests\n",
    "\n",
    "from lgn.engine import Trainer\n",
    "from lgn.engine import init_argparse, init_file_paths, init_logger, init_cuda, logging_printout, fix_args\n",
    "from lgn.engine import init_optimizer, init_scheduler\n",
    "from lgn.data.utils import initialize_datasets\n",
    "from lgn.cg_lib import CGDict\n",
    "\n",
    "from lgn.data.collate import collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes printing tensors more readable.\n",
    "torch.set_printoptions(linewidth=1000, threshold=100000)\n",
    "\n",
    "logger = logging.getLogger('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arguments -- Just\n",
    "args = init_argparse()\n",
    "\n",
    "# Initialize file paths\n",
    "args = init_file_paths(args)\n",
    "\n",
    "# Initialize logger\n",
    "init_logger(args)\n",
    "\n",
    "# Write input paramaters and paths to log\n",
    "logging_printout(args)\n",
    "\n",
    "# Fix possible inconsistencies in arguments\n",
    "args = fix_args(args)\n",
    "\n",
    "# Initialize device and data type\n",
    "device, dtype = init_cuda(args)\n",
    "\n",
    "# Initialize dataloder\n",
    "args, datasets = initialize_datasets(args, args.datadir, num_pts=None)\n",
    "\n",
    "if args.task.startswith('eval'):\n",
    "    args.load = True\n",
    "    args.num_epoch = 1\n",
    "\n",
    "# Construct PyTorch dataloaders from datasets\n",
    "collate = lambda data: collate_fn(data, scale=args.scale, nobj=args.nobj, add_beams=args.add_beams, beam_mass=args.beam_mass)\n",
    "dataloaders = {split: DataLoader(dataset,\n",
    "                                    batch_size=args.batch_size,\n",
    "                                    shuffle=args.shuffle if (split == 'train') else False,\n",
    "                                    num_workers=args.num_workers,\n",
    "                                    collate_fn=collate)\n",
    "                for split, dataset in datasets.items()}\n",
    "\n",
    "# Initialize model\n",
    "model = LGNTopTag(args.maxdim, args.max_zf, args.num_cg_levels, args.num_channels,\n",
    "                    args.cutoff_type, args.hard_cut_rad, args.soft_cut_rad, args.soft_cut_width,\n",
    "                    args.weight_init, args.level_gain, args.num_basis_fn,\n",
    "                    args.top, args.input, args.num_mpnn_levels, activation=args.activation, pmu_in=args.pmu_in, add_beams=args.add_beams,\n",
    "                    mlp=args.mlp, mlp_depth=args.mlp_depth, mlp_width=args.mlp_width,\n",
    "                    scale=1., full_scalars=args.full_scalars,\n",
    "                    device=device, dtype=dtype)\n",
    "\n",
    "if args.parallel:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Initialize the scheduler and optimizer\n",
    "optimizer = init_optimizer(args, model)\n",
    "scheduler, restart_epochs = init_scheduler(args, optimizer)\n",
    "\n",
    "# Define a loss function.\n",
    "# loss_fn = torch.nn.functional.cross_entropy\n",
    "loss_fn = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# Apply the covariance and permutation invariance tests.\n",
    "if args.test:\n",
    "    lgn_tests(model, dataloaders['train'], args, cg_dict=model.cg_dict)\n",
    "\n",
    "# Instantiate the training class\n",
    "trainer = Trainer(args, dataloaders, model, loss_fn, optimizer, scheduler, restart_epochs, device, dtype)\n",
    "\n",
    "# Load from checkpoint file. If no checkpoint file exists, automatically does nothing.\n",
    "trainer.load_checkpoint()\n",
    "\n",
    "# Train model.\n",
    "trainer.train()\n",
    "\n",
    "# Test predictions on best model and also last checkpointed model.\n",
    "trainer.evaluate(splits=['test'])"
   ]
  }
 ]
}